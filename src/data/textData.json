{
  "header": {
    "ru": "SWE-MERA — это динамичная альтернатива SWE-bench",
    "eng": "SWE-MERA – a dynamic alternative to SWE-bench"
  },

  "home": {
    "ru": "Главная",
    "eng": "Home"
  },

  "leaderboard": {
    "ru": "Лидерборд",
    "eng": "Leaderboard"
  },

  "headerBlockFirst": {
    "ru": "Описание",
    "eng": "Description"
  },

  "textBlockFirst": {
    "ru": "<b>SWE-Mera</b> — это динамичная альтернатива SWE-bench, которая обходит риск утечки данных благодаря регулярному добавлению новых свежих примеров. Процесс сбора данных тщательно отбирает задачи из реальных open-source репозиториев GitHub, фильтрует их при помощи LLM-as-a-judge и проверяет решения и тесты с использованием надежного тестового фреймворка. Кроме того, лидерборд <b>SWE-Mera</b> позволяет выбирать задачи из разных временных периодов, что облегчает обнаружение моделей, подвергшихся контаминации. <p>Бенчмарки по типу SWE-bench созданы для оценки ИИ-систем в решении issues с реальных репозиториев. Каждая задача сопровождается эталонным решением и тестовым патчем, который позволяет убедиться в корректности решения: тесты, входящие в такой патч, должны падать до применения решения и успешно проходить после. SWE-bench стал стандартом для оценки больших языковых моделей в программировании, особенно его SWE-bench/Lite и SWE-bench/Verified версии. Однако, поскольку бенчмарк статичен, а задачи и их решения доступны в открытом доступе, возрастает риск утечки данных.</p>",
    "eng": "<b>SWE-Mera</b> is a dynamic alternative to SWE-bench that mitigates the risk of data contamination by regularly incorporating new, up-to-date examples. The data collection pipeline rigorously selects issues from real open-source GitHub repositories, filters them with an LLM-as-a-judge approach, and verifies solutions and tests using a robust test framework. Additionally, the <b>SWE-MERA</b> leaderboard lets users choose issues from different time periods, making it easier to detect models affected by data contamination. <p>Benchmarks like SWE-bench are designed to evaluate the ability of AI systems to solve issues from real repositories. Each task comes with a reference solution and a test patch that allows one to verify correctness of the solution: the tests in this patch should fail before the solution is applied and successfully pass afterwards. SWE-bench has become a standard for evaluating large language models (LLMs) on programming tasks, particularly through its SWE-bench/Lite and SWE-bench/Verified subsets. However, since the benchmark is static and both the tasks and their solutions are publicly available, the risk of data contamination increases.</p>"
  },

  "model": {
    "ru": "Модель",
    "eng": "Model"
  },

  "position": {
    "ru": "Позиция",
    "eng": "Position"
  },

  "tasks": {
    "ru": "Задачи",
    "eng": "Tasks"
  },

  "trajectory": {
    "ru": "Ссылка",
    "eng": "Trajectory"
  },

  "notExistedData": {
    "ru": "Данных за этот период нет",
    "eng": "No data for this period"
  }
}
